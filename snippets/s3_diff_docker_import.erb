<%
base_image = "/opt/dockly/base_image.tar"
%>

s3_diff_docker_import_base_fn() {
  s3_path="<%= data[:base_image] %>"
  log "fetch: starting to fetch $s3_path"
  # TODO: Remove timeout once https://github.com/aws/aws-cli/issues/1178 closes
  timeout 600 aws s3 cp --quiet $s3_path - 2> >(log)
  log "fetch: successfully fetched $s3_path"
}

s3_diff_docker_import_diff_fn() {
  s3_path="<%= data[:diff_image] %>"
  log "fetch: starting to fetch $s3_path"
  timeout 600 aws s3 cp --quiet $s3_path - 2> >(log)
  log "fetch: successfully fetched $s3_path"
}

base_image() {
  s3_diff_docker_import_base_fn | gunzip -vc > "<%= base_image %>" 2> >(log)
}

stream_image() {
  size=$(stat --format "%s" "<%= base_image %>")
  head_size=$(($size - 1024))
  head -c $head_size "<%= base_image %>"
  s3_diff_docker_import_diff_fn | (gunzip -vc 2> >(log) || fatal "tardiff failed to gunzip")
}

docker_import() {
  repo=<%= data[:repo] %>
  tag=<%= data[:tag] %>
  docker import - $repo:$tag > >(log) 2>&1 || fatal "docker failed to import"
}

remove_bad_imports() {
  docker images | grep \<none | awk '{ print $3 }' | xargs docker rmi
}

worked=1
for attempt in {1..200}; do
  [[ $worked != 0 ]] || break
  base_image && worked=0 || (log "fetch: attempt $attempt failed, sleeping 30"; sleep 30)
done
[[ $worked != 0 ]] && fatal "fetch: failed to pull base image"
log "fetch: successfully pulled base image"

worked=1
for attempt in {1..200}; do
  [[ $worked != 0 ]] || break
  stream_image | docker_import && worked=0 || (log "fetch: attempt $attempt failed, sleeping 30"; sleep 30)
  [[ $worked != 0 ]] && remove_bad_imports
done
[[ $worked != 0 ]] && fatal "fetch: failed to import diff image"
log "fetch: successfully imported diff image"
